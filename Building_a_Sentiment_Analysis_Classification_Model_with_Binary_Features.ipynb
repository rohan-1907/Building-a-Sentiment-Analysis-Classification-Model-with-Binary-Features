{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnPcmLQW0LUw",
    "outputId": "1c1d49c7-9cc3-4d85-e96e-60b89c1bc9b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\ivopb\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stands for Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import OS Library\n",
    "import os\n",
    "\n",
    "# Import string library\n",
    "import string\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "# Supress warnings\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import math\n",
    "import math\n",
    "\n",
    "# Import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0GyafdA0LU5"
   },
   "source": [
    "## 1 - Preparing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J3U0knt0LU6"
   },
   "source": [
    "### 1.1 - Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSEUrvKu0LU7"
   },
   "source": [
    "**First we will load 1000 positive reviews and 1000 negative reviews:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJLaMjhG0LU9"
   },
   "outputs": [],
   "source": [
    "neg_files = os.listdir('data/neg')[0:1000]\n",
    "pos_files = os.listdir('data/pos')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPLNhMZp0LU-"
   },
   "outputs": [],
   "source": [
    "def get_reviews(directory, file_list):\n",
    "    files=[]\n",
    "    for file in file_list:\n",
    "        f = open('data/'+directory+'/'+file, \"r\")\n",
    "        #append each line in the file to a list\n",
    "        files.append(f.read())\n",
    "        f.close()\n",
    "        \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXsE-t8K0LVA"
   },
   "outputs": [],
   "source": [
    "negative_files = get_reviews('neg', neg_files)\n",
    "positive_files = get_reviews('pos', pos_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99OBQ3s80LVB"
   },
   "source": [
    "Let's see some examples of some negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWiEdFVo0LVB",
    "outputId": "7727abb4-4201-40ba-92e3-03f248c0b320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the happy bastard's quick movie review \n",
      "damn that y2k bug . \n",
      "it's got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \n",
      "little do they know the power within . . . \n",
      "going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \n",
      "we don't know why the crew was really out in the middle of nowhere , we don't know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don't know why donald sutherland is stumbling around drunkenly throughout . \n",
      "here , it's just \" hey , let's chase these people around with some robots \" . \n",
      "the acting is below average , even from the likes of curtis . \n",
      "you're more likely to get a kick out of her work in halloween h20 . \n",
      "sutherland is wasted and baldwin , well , he's acting like a baldwin , of course . \n",
      "the real star here are stan winston's robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone's brain . \n",
      "so , if robots and body parts really turn you on , here's your movie . \n",
      "otherwise , it's pretty much a sunken ship of a movie . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(negative_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmZFhwK60LVC",
    "outputId": "2353a259-b4d5-46c9-95d0-8194be24d60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is movies like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch . \n",
      "based on the late 1960's television show by the same name , the mod squad tells the tale of three reformed criminals under the employ of the police to go undercover . \n",
      "however , things go wrong as evidence gets stolen and they are immediately under suspicion . \n",
      "of course , the ads make it seem like so much more . \n",
      "quick cuts , cool music , claire dane's nice hair and cute outfits , car chases , stuff blowing up , and the like . \n",
      "sounds like a cool movie , does it not ? \n",
      "after the first fifteen minutes , it quickly becomes apparent that it is not . \n",
      "the mod squad is certainly a slick looking production , complete with nice hair and costumes , but that simply isn't enough . \n",
      "the film is best described as a cross between an hour-long cop show and a music video , both stretched out into the span of an hour and a half . \n",
      "and with it comes every single clich ? . \n",
      "it doesn't really matter that the film is based on a television show , as most of the plot elements have been recycled from everything we've already seen . \n",
      "the characters and acting is nothing spectacular , sometimes even bordering on wooden . \n",
      "claire danes and omar epps deliver their lines as if they are bored , which really transfers onto the audience . \n",
      "the only one to escape relatively unscathed is giovanni ribisi , who plays the resident crazy man , ultimately being the only thing worth watching . \n",
      "unfortunately , even he's not enough to save this convoluted mess , as all the characters don't do much apart from occupying screen time . \n",
      "with the young cast , cool clothes , nice hair , and hip soundtrack , it appears that the film is geared towards the teenage mindset . \n",
      "despite an american 'r' rating ( which the content does not justify ) , the film is way too juvenile for the older mindset . \n",
      "information on the characters is literally spoon-fed to the audience ( would it be that hard to show us instead of telling us ? ) , dialogue is poorly written , and the plot is extremely predictable . \n",
      "the way the film progresses , you likely won't even care if the heroes are in any jeopardy , because you'll know they aren't . \n",
      "basing the show on a 1960's television show that nobody remembers is of questionable wisdom , especially when one considers the target audience and the fact that the number of memorable films based on television shows can be counted on one hand ( even one that's missing a finger or two ) . \n",
      "the number of times that i checked my watch ( six ) is a clear indication that this film is not one of them . \n",
      "it is clear that the film is nothing more than an attempt to cash in on the teenage spending dollar , judging from the rash of really awful teen-flicks that we've been seeing as of late . \n",
      "avoid this film at all costs . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(negative_files[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbEUCzs50LVD"
   },
   "source": [
    "And some positive ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgnbwFZQ0LVE",
    "outputId": "5279acd8-0f88-4e64-a8e1-8ab443c39c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you've got mail works alot better than it deserves to . \n",
      "in order to make the film a success , all they had to do was cast two extremely popular and attractive stars , have them share the screen for about two hours and then collect the profits . \n",
      "no real acting was involved and there is not an original or inventive bone in it's body ( it's basically a complete re-shoot of the shop around the corner , only adding a few modern twists ) . \n",
      "essentially , it goes against and defies all concepts of good contemporary filmmaking . \n",
      "it's overly sentimental and at times terribly mushy , not to mention very manipulative . \n",
      "but oh , how enjoyable that manipulation is . \n",
      "but there must be something other than the casting and manipulation that makes the movie work as well as it does , because i absolutely hated the previous ryan/hanks teaming , sleepless in seattle . \n",
      "it couldn't have been the directing , because both films were helmed by the same woman . \n",
      "i haven't quite yet figured out what i liked so much about you've got mail , but then again , is that really important ? \n",
      "if you like something so much , why even question it ? \n",
      "again , the storyline is as cliched as they come . \n",
      "tom hanks plays joe fox , the insanely likeable owner of a discount book chain and meg ryan plays kathleen kelley , the even more insanely likeable proprietor of a family-run children's book shop called , in a nice homage , the shop around the corner . \n",
      "fox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business . \n",
      "little do they know , they are already in love with each other over the internet , only neither party knows the other person's true identity . \n",
      "the rest of the story isn't important because all it does is serve as a mere backdrop for the two stars to share the screen . \n",
      "sure , there are some mildly interesting subplots , but they all fail in comparison to the utter cuteness of the main relationship . \n",
      "all of this , of course , leads up to the predictable climax . \n",
      "but as foreseeable as the ending is , it's so damn cute and well-done that i doubt any movie in the entire year contains a scene the evokes as much pure joy as this part does . \n",
      "when ryan discovers the true identity of her online love , i was filled with such , for lack of a better word , happiness that for the first time all year , i actually left the theater smiling . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(positive_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZSwqtgi0LVF",
    "outputId": "2ff5cf1d-f2d4-42ec-9bb0-93618da84893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody's surprise ( perhaps even the studio ) the film becomes a critical darling . \n",
      "mtv films' _election , a high school comedy starring matthew broderick and reese witherspoon , is a current example . \n",
      "did anybody know this film existed a week before it opened ? \n",
      "the plot is deceptively simple . \n",
      "george washington carver high school is having student elections . \n",
      "tracy flick ( reese witherspoon ) is an over-achiever with her hand raised at nearly every question , way , way , high . \n",
      "mr . \" m \" ( matthew broderick ) , sick of the megalomaniac student , encourages paul , a popular-but-slow jock to run . \n",
      "and paul's nihilistic sister jumps in the race as well , for personal reasons . \n",
      "the dark side of such sleeper success is that , because expectations were so low going in , the fact that this was quality stuff made the reviews even more enthusiastic than they have any right to be . \n",
      "you can't help going in with the baggage of glowing reviews , which is in contrast to the negative baggage that the reviewers were likely to have . \n",
      "_election , a good film , does not live up to its hype . \n",
      "what makes _election_ so disappointing is that it contains significant plot details lifted directly from _rushmore_ , released a few months earlier . \n",
      "the similarities are staggering : \n",
      "tracy flick ( _election_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \n",
      "max fischer ( _rushmore_ ) is the president of an extraordinary number of clubs , and is involved with the school play . \n",
      "the most significant tension of _election_ is the potential relationship between a teacher and his student . \n",
      "the most significant tension of _rushmore_ is the potential relationship between a teacher and his student . \n",
      "tracy flick is from a single parent home , which has contributed to her drive . \n",
      "max fischer is from a single parent home , which has contributed to his drive . \n",
      "the male bumbling adult in _election_ ( matthew broderick ) pursues an extramarital affair , gets caught , and his whole life is ruined . \n",
      "he even gets a bee sting . \n",
      "the male bumbling adult in _rushmore_ ( bill murray ) pursues an extramarital affair , gets caught , and his whole life is ruined . \n",
      "he gets several bee stings . \n",
      "and so on . \n",
      "what happened ? \n",
      "how is it that an individual screenplay ( _rushmore_ ) and a novel ( _election_ ) contain so many significant plot points , and yet both films were probably not even aware of each other , made from two different studios , from a genre ( the high school geeks revenge movie ) that hadn't been fully formed yet ? \n",
      "even so , the strengths of _election_ rely upon its fantastic performances from broderick , witherspoon , and newcomer jessica campbell , as paul's anti-social sister , tammy . \n",
      "broderick here is playing the mr . rooney role from _ferris bueller_ , and he seems to be having the most fun he's had since then . \n",
      "witherspoon is a revelation . \n",
      "it's early in the year , it's a comedy , and teenagers have little clout , but for my money , witherspoon deserves an oscar nomination . \n",
      "and once campbell's character gets going , like in her fantastic speech in the gymnasium , then you're won over . \n",
      "one thing that's been bothering me since i've seen it . \n",
      "there is an extraordinary amount of sexuality in this film . \n",
      "i suppose that , coming from mtv films , i should expect no less . . . \n",
      "but the film starts off light and airy , like a sitcom . \n",
      "as the screws tighten , and the tensions mount , alexander payne decides to add elements that , frankly , distract from the story . \n",
      "it is bad enough that mr . m doesn't like tracy's determination to win at all costs , but did they have to throw in the student/teacher relationship ? \n",
      "even so , there's no logical reason why mr . m has an affair when he does . \n",
      "there's a lot to like in _election_ , but the plot similarities to _rushmore_ , and the tonal nosedive it takes as it gets explicitly sex-driven , mark this as a disappointment . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(positive_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5aeq7Tu0LVG"
   },
   "source": [
    "### 1.2 - Pre-process Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqTTsosu0LVG"
   },
   "source": [
    "it's relevant to pre-process our text with some of the techniques we've learned so far. Here's our pipeline:\n",
    "    - Lower case text.\n",
    "    - Remove punctuation\n",
    "    - Tokenize our reviews.\n",
    "    - Remove stop words.\n",
    "    - Stem our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP8iDvd_0LVH"
   },
   "outputs": [],
   "source": [
    "positive_files =[file.lower() for file in positive_files]\n",
    "negative_files =[file.lower() for file in negative_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1hmrl-Y0LVI"
   },
   "source": [
    "Let's build our function that removes the punctuation from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz18cto20LVJ"
   },
   "outputs": [],
   "source": [
    "def remove_punct(list_of_text):\n",
    "    \n",
    "    no_punct = []\n",
    "    \n",
    "    for index, file in enumerate(list_of_text):\n",
    "        file = (\n",
    "            file.translate(\n",
    "                str.maketrans('', '', string.punctuation)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        no_punct.append(file)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kndLpRZ10LVK"
   },
   "outputs": [],
   "source": [
    "positive_files = remove_punct(positive_files)\n",
    "negative_files = remove_punct(negative_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhFfOfYY0LVL"
   },
   "source": [
    "Removing our stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wUw-Sbu0LVM"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_68f15r0LVO",
    "outputId": "05085ebd-f122-4868-9b9a-38884d448719"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGLFn10I0LVO"
   },
   "outputs": [],
   "source": [
    "positive_files_tokenized = [word_tokenize(file) for file in positive_files]\n",
    "negative_files_tokenized = [word_tokenize(file) for file in negative_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNH70iAb0LVO"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokenized_text):\n",
    "    # Retrieve Stop Words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    list_files_no_stop = []\n",
    "    \n",
    "    for file in tokenized_text:\n",
    "        no_stop_words = []\n",
    "        for word in file:\n",
    "            if word not in stop_words:\n",
    "                no_stop_words.append(word)\n",
    "        \n",
    "        list_files_no_stop.append(\n",
    "            no_stop_words\n",
    "        )\n",
    "        \n",
    "    return list_files_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q48tTAp0LVP"
   },
   "outputs": [],
   "source": [
    "positive_files_tokenized = remove_stop_words(positive_files_tokenized)\n",
    "negative_files_tokenized = remove_stop_words(negative_files_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R7WoSX40LVP"
   },
   "source": [
    "Before stemming, let's do some analytics on our positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAS1F1gQ0LVQ"
   },
   "outputs": [],
   "source": [
    "# Let's flat our list into words:\n",
    "flat_list_positive = [\n",
    "    item for sublist in positive_files_tokenized for item in sublist\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsWMFStM0LVQ",
    "outputId": "18debae8-82a6-40c6-99d3-a841aedb6f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 374641 words in our positive files - 34781 are distinct\n"
     ]
    }
   ],
   "source": [
    "print('We have {} words in our positive files - {} are distinct'\n",
    "      .format(len(flat_list_positive), len(set(flat_list_positive))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myRszsLW0LVR"
   },
   "outputs": [],
   "source": [
    "# Let's flat our list into words:\n",
    "flat_list_negative = [\n",
    "    item for sublist in negative_files_tokenized for item in sublist\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt2NsO-Y0LVS",
    "outputId": "b82a7b87-9f04-4ebe-f305-68dedb5f1f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 335151 words in our positive files - 32595 are distinct\n"
     ]
    }
   ],
   "source": [
    "print('We have {} words in our positive files - {} are distinct'\n",
    "      .format(len(flat_list_negative), len(set(flat_list_negative))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCrh-CeE0LVS"
   },
   "source": [
    "### 1.3 - Log Ratio of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzsHUWRa0LVT"
   },
   "source": [
    "We can rely on Freq Dist to check the most relevant words for positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBuSs1RH0LVT",
    "outputId": "0253078e-bd72-4a05-f424-f754cae290fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 3994),\n",
       " ('movie', 3048),\n",
       " ('one', 2617),\n",
       " ('like', 1835),\n",
       " ('even', 1381),\n",
       " ('good', 1127),\n",
       " ('time', 1112),\n",
       " ('would', 1049),\n",
       " ('get', 1037),\n",
       " ('bad', 1019),\n",
       " ('much', 997),\n",
       " ('characters', 909),\n",
       " ('films', 903),\n",
       " ('story', 903),\n",
       " ('character', 893),\n",
       " ('plot', 876),\n",
       " ('two', 865),\n",
       " ('make', 818),\n",
       " ('first', 805),\n",
       " ('really', 787)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(flat_list_negative).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAB6s3kj0LVU",
    "outputId": "b530191a-d71d-4940-d162-e38d1ccf960b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 4855),\n",
       " ('one', 2903),\n",
       " ('movie', 2392),\n",
       " ('like', 1716),\n",
       " ('story', 1212),\n",
       " ('films', 1198),\n",
       " ('also', 1197),\n",
       " ('good', 1191),\n",
       " ('even', 1174),\n",
       " ('time', 1171),\n",
       " ('characters', 1038),\n",
       " ('much', 1024),\n",
       " ('character', 1010),\n",
       " ('would', 993),\n",
       " ('life', 983),\n",
       " ('well', 967),\n",
       " ('first', 963),\n",
       " ('two', 960),\n",
       " ('see', 952),\n",
       " ('way', 913)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(flat_list_positive).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4bn-WGo0LVV"
   },
   "outputs": [],
   "source": [
    "negative_freq = pd.DataFrame(\n",
    "    nltk.FreqDist(flat_list_negative).values(), \n",
    "    index=nltk.FreqDist(flat_list_negative).keys(),\n",
    "    columns=['word_count_negative']\n",
    ")\n",
    "positive_freq = pd.DataFrame(\n",
    "    nltk.FreqDist(flat_list_positive).values(), \n",
    "    index=nltk.FreqDist(flat_list_positive).keys(),\n",
    "    columns=['word_count_positive']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWEGRWID0LVV"
   },
   "outputs": [],
   "source": [
    "word_sentiment = negative_freq.join(positive_freq).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dat2i7vA0LVW"
   },
   "source": [
    "Let's calculate the Ratio between Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPNRVdsT0LVW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_sentiment['ratio'] = (\n",
    "    np.log(\n",
    "        word_sentiment.word_count_positive\n",
    "        /word_sentiment.word_count_negative\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvloWnqr0LVW"
   },
   "source": [
    "We can now check which words are more relevant for positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZKninRK0LVX",
    "outputId": "72e7d02b-b518-4026-a34f-ac4140df96d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_negative</th>\n",
       "      <th>word_count_positive</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>godzilla</th>\n",
       "      <td>116</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-2.114533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wasted</th>\n",
       "      <td>101</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-1.842532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lame</th>\n",
       "      <td>90</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>121</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-1.704748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>259</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.685627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jackie</th>\n",
       "      <td>47</td>\n",
       "      <td>209.0</td>\n",
       "      <td>1.492187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cameron</th>\n",
       "      <td>22</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.572397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toy</th>\n",
       "      <td>17</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.632695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titanic</th>\n",
       "      <td>18</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.763589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truman</th>\n",
       "      <td>11</td>\n",
       "      <td>121.0</td>\n",
       "      <td>2.397895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1210 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word_count_negative  word_count_positive     ratio\n",
       "godzilla                  116                 14.0 -2.114533\n",
       "wasted                    101                 16.0 -1.842532\n",
       "lame                       90                 15.0 -1.791759\n",
       "waste                     121                 22.0 -1.704748\n",
       "worst                     259                 48.0 -1.685627\n",
       "...                       ...                  ...       ...\n",
       "jackie                     47                209.0  1.492187\n",
       "cameron                    22                106.0  1.572397\n",
       "toy                        17                 87.0  1.632695\n",
       "titanic                    18                105.0  1.763589\n",
       "truman                     11                121.0  2.397895\n",
       "\n",
       "[1210 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment.loc[\n",
    "    word_sentiment.word_count_negative + word_sentiment.word_count_positive > 100 \n",
    "].sort_values(by='ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHDTfrpF0LVX",
    "outputId": "4e434f60-35ef-4736-ced8-489191ab7d7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films',\n",
       " 'adapted',\n",
       " 'comic',\n",
       " 'books',\n",
       " 'plenty',\n",
       " 'success',\n",
       " 'whether',\n",
       " 'theyre',\n",
       " 'superheroes',\n",
       " 'batman',\n",
       " 'superman',\n",
       " 'spawn',\n",
       " 'geared',\n",
       " 'toward',\n",
       " 'kids',\n",
       " 'casper',\n",
       " 'arthouse',\n",
       " 'crowd',\n",
       " 'ghost',\n",
       " 'world',\n",
       " 'theres',\n",
       " 'never',\n",
       " 'really',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'like',\n",
       " 'hell',\n",
       " 'starters',\n",
       " 'created',\n",
       " 'alan',\n",
       " 'moore',\n",
       " 'eddie',\n",
       " 'campbell',\n",
       " 'brought',\n",
       " 'medium',\n",
       " 'whole',\n",
       " 'new',\n",
       " 'level',\n",
       " 'mid',\n",
       " '80s',\n",
       " '12part',\n",
       " 'series',\n",
       " 'called',\n",
       " 'watchmen',\n",
       " 'say',\n",
       " 'moore',\n",
       " 'campbell',\n",
       " 'thoroughly',\n",
       " 'researched',\n",
       " 'subject',\n",
       " 'jack',\n",
       " 'ripper',\n",
       " 'would',\n",
       " 'like',\n",
       " 'saying',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'starting',\n",
       " 'look',\n",
       " 'little',\n",
       " 'odd',\n",
       " 'book',\n",
       " 'graphic',\n",
       " 'novel',\n",
       " '500',\n",
       " 'pages',\n",
       " 'long',\n",
       " 'includes',\n",
       " 'nearly',\n",
       " '30',\n",
       " 'consist',\n",
       " 'nothing',\n",
       " 'footnotes',\n",
       " 'words',\n",
       " 'dont',\n",
       " 'dismiss',\n",
       " 'film',\n",
       " 'source',\n",
       " 'get',\n",
       " 'past',\n",
       " 'whole',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'thing',\n",
       " 'might',\n",
       " 'find',\n",
       " 'another',\n",
       " 'stumbling',\n",
       " 'block',\n",
       " 'hells',\n",
       " 'directors',\n",
       " 'albert',\n",
       " 'allen',\n",
       " 'hughes',\n",
       " 'getting',\n",
       " 'hughes',\n",
       " 'brothers',\n",
       " 'direct',\n",
       " 'seems',\n",
       " 'almost',\n",
       " 'ludicrous',\n",
       " 'casting',\n",
       " 'carrot',\n",
       " 'top',\n",
       " 'well',\n",
       " 'anything',\n",
       " 'riddle',\n",
       " 'better',\n",
       " 'direct',\n",
       " 'film',\n",
       " 'thats',\n",
       " 'set',\n",
       " 'ghetto',\n",
       " 'features',\n",
       " 'really',\n",
       " 'violent',\n",
       " 'street',\n",
       " 'crime',\n",
       " 'mad',\n",
       " 'geniuses',\n",
       " 'behind',\n",
       " 'menace',\n",
       " 'ii',\n",
       " 'society',\n",
       " 'ghetto',\n",
       " 'question',\n",
       " 'course',\n",
       " 'whitechapel',\n",
       " '1888',\n",
       " 'londons',\n",
       " 'east',\n",
       " 'end',\n",
       " 'filthy',\n",
       " 'sooty',\n",
       " 'place',\n",
       " 'whores',\n",
       " 'called',\n",
       " 'unfortunates',\n",
       " 'starting',\n",
       " 'get',\n",
       " 'little',\n",
       " 'nervous',\n",
       " 'mysterious',\n",
       " 'psychopath',\n",
       " 'carving',\n",
       " 'profession',\n",
       " 'surgical',\n",
       " 'precision',\n",
       " 'first',\n",
       " 'stiff',\n",
       " 'turns',\n",
       " 'copper',\n",
       " 'peter',\n",
       " 'godley',\n",
       " 'robbie',\n",
       " 'coltrane',\n",
       " 'world',\n",
       " 'enough',\n",
       " 'calls',\n",
       " 'inspector',\n",
       " 'frederick',\n",
       " 'abberline',\n",
       " 'johnny',\n",
       " 'depp',\n",
       " 'blow',\n",
       " 'crack',\n",
       " 'case',\n",
       " 'abberline',\n",
       " 'widower',\n",
       " 'prophetic',\n",
       " 'dreams',\n",
       " 'unsuccessfully',\n",
       " 'tries',\n",
       " 'quell',\n",
       " 'copious',\n",
       " 'amounts',\n",
       " 'absinthe',\n",
       " 'opium',\n",
       " 'upon',\n",
       " 'arriving',\n",
       " 'whitechapel',\n",
       " 'befriends',\n",
       " 'unfortunate',\n",
       " 'named',\n",
       " 'mary',\n",
       " 'kelly',\n",
       " 'heather',\n",
       " 'graham',\n",
       " 'say',\n",
       " 'isnt',\n",
       " 'proceeds',\n",
       " 'investigate',\n",
       " 'horribly',\n",
       " 'gruesome',\n",
       " 'crimes',\n",
       " 'even',\n",
       " 'police',\n",
       " 'surgeon',\n",
       " 'cant',\n",
       " 'stomach',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'anyone',\n",
       " 'needs',\n",
       " 'briefed',\n",
       " 'jack',\n",
       " 'ripper',\n",
       " 'wont',\n",
       " 'go',\n",
       " 'particulars',\n",
       " 'say',\n",
       " 'moore',\n",
       " 'campbell',\n",
       " 'unique',\n",
       " 'interesting',\n",
       " 'theory',\n",
       " 'identity',\n",
       " 'killer',\n",
       " 'reasons',\n",
       " 'chooses',\n",
       " 'slay',\n",
       " 'comic',\n",
       " 'dont',\n",
       " 'bother',\n",
       " 'cloaking',\n",
       " 'identity',\n",
       " 'ripper',\n",
       " 'screenwriters',\n",
       " 'terry',\n",
       " 'hayes',\n",
       " 'vertical',\n",
       " 'limit',\n",
       " 'rafael',\n",
       " 'yglesias',\n",
       " 'les',\n",
       " 'mis',\n",
       " 'rables',\n",
       " 'good',\n",
       " 'job',\n",
       " 'keeping',\n",
       " 'hidden',\n",
       " 'viewers',\n",
       " 'end',\n",
       " 'funny',\n",
       " 'watch',\n",
       " 'locals',\n",
       " 'blindly',\n",
       " 'point',\n",
       " 'finger',\n",
       " 'blame',\n",
       " 'jews',\n",
       " 'indians',\n",
       " 'englishman',\n",
       " 'could',\n",
       " 'never',\n",
       " 'capable',\n",
       " 'committing',\n",
       " 'ghastly',\n",
       " 'acts',\n",
       " 'hells',\n",
       " 'ending',\n",
       " 'whistling',\n",
       " 'stonecutters',\n",
       " 'song',\n",
       " 'simpsons',\n",
       " 'days',\n",
       " 'holds',\n",
       " 'back',\n",
       " 'electric',\n",
       " 'carwho',\n",
       " 'made',\n",
       " 'steve',\n",
       " 'guttenberg',\n",
       " 'star',\n",
       " 'dont',\n",
       " 'worry',\n",
       " 'itll',\n",
       " 'make',\n",
       " 'sense',\n",
       " 'see',\n",
       " 'onto',\n",
       " 'hells',\n",
       " 'appearance',\n",
       " 'certainly',\n",
       " 'dark',\n",
       " 'bleak',\n",
       " 'enough',\n",
       " 'surprising',\n",
       " 'see',\n",
       " 'much',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'tim',\n",
       " 'burton',\n",
       " 'film',\n",
       " 'planet',\n",
       " 'apes',\n",
       " 'times',\n",
       " 'seems',\n",
       " 'like',\n",
       " 'sleepy',\n",
       " 'hollow',\n",
       " '2',\n",
       " 'print',\n",
       " 'saw',\n",
       " 'wasnt',\n",
       " 'completely',\n",
       " 'finished',\n",
       " 'color',\n",
       " 'music',\n",
       " 'finalized',\n",
       " 'comments',\n",
       " 'marilyn',\n",
       " 'manson',\n",
       " 'cinematographer',\n",
       " 'peter',\n",
       " 'deming',\n",
       " 'dont',\n",
       " 'say',\n",
       " 'word',\n",
       " 'ably',\n",
       " 'captures',\n",
       " 'dreariness',\n",
       " 'victorianera',\n",
       " 'london',\n",
       " 'helped',\n",
       " 'make',\n",
       " 'flashy',\n",
       " 'killing',\n",
       " 'scenes',\n",
       " 'remind',\n",
       " 'crazy',\n",
       " 'flashbacks',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'even',\n",
       " 'though',\n",
       " 'violence',\n",
       " 'film',\n",
       " 'pales',\n",
       " 'comparison',\n",
       " 'blackandwhite',\n",
       " 'comic',\n",
       " 'oscar',\n",
       " 'winner',\n",
       " 'martin',\n",
       " 'childs',\n",
       " 'shakespeare',\n",
       " 'love',\n",
       " 'production',\n",
       " 'design',\n",
       " 'turns',\n",
       " 'original',\n",
       " 'prague',\n",
       " 'surroundings',\n",
       " 'one',\n",
       " 'creepy',\n",
       " 'place',\n",
       " 'even',\n",
       " 'acting',\n",
       " 'hell',\n",
       " 'solid',\n",
       " 'dreamy',\n",
       " 'depp',\n",
       " 'turning',\n",
       " 'typically',\n",
       " 'strong',\n",
       " 'performance',\n",
       " 'deftly',\n",
       " 'handling',\n",
       " 'british',\n",
       " 'accent',\n",
       " 'ians',\n",
       " 'holm',\n",
       " 'joe',\n",
       " 'goulds',\n",
       " 'secret',\n",
       " 'richardson',\n",
       " '102',\n",
       " 'dalmatians',\n",
       " 'log',\n",
       " 'great',\n",
       " 'supporting',\n",
       " 'roles',\n",
       " 'big',\n",
       " 'surprise',\n",
       " 'graham',\n",
       " 'cringed',\n",
       " 'first',\n",
       " 'time',\n",
       " 'opened',\n",
       " 'mouth',\n",
       " 'imagining',\n",
       " 'attempt',\n",
       " 'irish',\n",
       " 'accent',\n",
       " 'actually',\n",
       " 'wasnt',\n",
       " 'half',\n",
       " 'bad',\n",
       " 'film',\n",
       " 'however',\n",
       " 'good',\n",
       " '2',\n",
       " '00',\n",
       " 'r',\n",
       " 'strong',\n",
       " 'violencegore',\n",
       " 'sexuality',\n",
       " 'language',\n",
       " 'drug',\n",
       " 'content']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_files_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXuiGfKz0LVY"
   },
   "source": [
    "Finally we can Stem our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOuR__bS0LVY"
   },
   "outputs": [],
   "source": [
    "def stem_sentence(tokenized_sentence):\n",
    "    # Retrieve Stop Words\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        stem_sentence.append(stemmer.stem(word))\n",
    "        \n",
    "    return stem_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKxeGPTX0LVY"
   },
   "outputs": [],
   "source": [
    "stemmed_negative_files = [\n",
    "    stem_sentence(sentence) for sentence in negative_files_tokenized\n",
    "]\n",
    "stemmed_positive_files = [\n",
    "    stem_sentence(sentence) for sentence in positive_files_tokenized\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9EiWrJZ0LVZ"
   },
   "source": [
    "Join our sentences into a single text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPRWpZHI0LVZ"
   },
   "outputs": [],
   "source": [
    "sentence_negative = [' '.join(file) for file in stemmed_negative_files]\n",
    "sentence_positive = [' '.join(file) for file in stemmed_positive_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNVeRPFZ0LVZ"
   },
   "outputs": [],
   "source": [
    "sentence_negative = pd.DataFrame(sentence_negative, columns=['sentence'])\n",
    "sentence_positive = pd.DataFrame(sentence_positive, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XryeuhE50LVa",
    "outputId": "c02991a7-9d51-4e9a-8642-83a5ad10df07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film adapt comic book plenti success whether t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>everi movi come along suspect studio everi ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youv got mail work alot better deserv order ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaw rare film grab attent show singl imag scre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemak lot like gener manag nfl team postsal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>june 30 1960 selftaught idealist yet pragmat y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>appar director toni kay major battl new line r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>one colleagu surpris told will see betsi wed s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bloodi clash independ lumumba refus pander bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>american action film slowli drown death sea as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  film adapt comic book plenti success whether t...\n",
       "1  everi movi come along suspect studio everi ind...\n",
       "2  youv got mail work alot better deserv order ma...\n",
       "3  jaw rare film grab attent show singl imag scre...\n",
       "4  moviemak lot like gener manag nfl team postsal...\n",
       "5  june 30 1960 selftaught idealist yet pragmat y...\n",
       "6  appar director toni kay major battl new line r...\n",
       "7  one colleagu surpris told will see betsi wed s...\n",
       "8  bloodi clash independ lumumba refus pander bel...\n",
       "9  american action film slowli drown death sea as..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_positive.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UEWlzn20LVa"
   },
   "outputs": [],
   "source": [
    "training_base = sentence_positive.append(sentence_negative).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAsr0c7N0LVa"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "X = cv.fit_transform(training_base.sentence).todense()\n",
    "X = pd.DataFrame(\n",
    "    X,\n",
    "    columns=cv.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMTnRn220LVb",
    "outputId": "c9c5d41a-dd2b-4984-f388-0c560c76a073"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 31903)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_-WX2-O0LVb"
   },
   "source": [
    "Our y consists of labels with zeros and one's - 1 for positive reviews and 0 for negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY7pu0FI0LVc"
   },
   "outputs": [],
   "source": [
    "y = np.append(np.ones(1000), np.zeros(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VisMVoI_0LVy",
    "outputId": "6ef08aa9-c305-4a6c-865b-084ff5fa645f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['00', '000', '0009f', '000acr', '000aweek', '000foot', '000paltri',\n",
       "       '007', '007esqu', '010',\n",
       "       ...\n",
       "       'zuehlk', 'zuko', 'zukovski', 'zulu', 'zundel', 'zurg', 'zweibel',\n",
       "       'zwick', 'zwigoff', 'zyci'],\n",
       "      dtype='object', length=31903)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTIqmGMg0LVy"
   },
   "source": [
    "### 2.2 Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxQUpgN10LV0"
   },
   "outputs": [],
   "source": [
    "X_test = X[0:100].append(X[-100:])\n",
    "y_test = np.append(y[0:100],y[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT6JZTNC0LV0"
   },
   "source": [
    "And now selecting our training set, which will contain 1800 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64fukWCi0LV0"
   },
   "outputs": [],
   "source": [
    "X_train = X[100:len(X)-100]\n",
    "y_train = y[100:len(y)-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn3U6N5D0LV1"
   },
   "source": [
    "Note that with our huge dimensional space we have a high probability of overfitting our algorithm. Let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lC2Ncr-0LV1"
   },
   "outputs": [],
   "source": [
    "lm = LogisticRegression(random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixdg43IA0LV1",
    "outputId": "4444a7fa-4f3a-4cab-e2f2-62c0418a5632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=1234, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8jx0uL80LV2"
   },
   "source": [
    "Let's check the accuracy on our train_set - we can use the predict method to predict using our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18NqcPkh0LV2"
   },
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_SaOtck0LV3",
    "outputId": "a1fa8b3d-271e-4824-b19b-4faeaecff41d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_pred == y_train).sum())/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnzaV0FF0LV3"
   },
   "source": [
    "Our accuracy is 100% on the training set. This is a result that will probably show us that we are overfitting our hypothesis - meaning that the function that we predicted (the weights) adjust perfectly to our training set. We are able to predict with 100% confidence the fact that a review is positive or negative.\n",
    "<br>\n",
    "<br>\n",
    "To really evaluate the performance of our algorithm, we have to check the accuracy in the test set - the data that we left out of this training process and that mimics the real world that your algorithm should find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmwFPBH20LV3"
   },
   "outputs": [],
   "source": [
    "y_pred_test = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aXk3QDV0LV4",
    "outputId": "f33b23a5-5621-416b-c58f-a7849e4a8366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_test == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kXoo-yM0LV4"
   },
   "source": [
    "As you can see, our model behaves generally well but in the test set we achieve 87% accuracy - still a lot better than our first model with two features but with a difference of 0.13 percentage points in accuracy between train and test sets. \n",
    "<br>\n",
    "<br>\n",
    "Having this difference between training and test datasets is not optimum as it may lead to a quicker model degradation (i.e., when you have your model in production, the accuracy may get worse over time really quickly). To reduce the overfit problem, we could use a technique called regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO1rEoRO0LV5",
    "outputId": "1672c552-8586-4f6a-c8ae-4b916c7e6d98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_reg = LogisticRegression(penalty='l2', C=0.001)\n",
    "lm_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygCGI1cS0LV5",
    "outputId": "2d938f31-2778-44c8-c388-ea3a3178918b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9016666666666666"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lm_reg.predict(X_train)\n",
    "((y_pred == y_train).sum())/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uLSSrQL0LV7",
    "outputId": "bf9d5f9a-4169-4328-8354-df29db54bddb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = lm_reg.predict(X_test)\n",
    "(y_pred_test == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yh3sMt_0LV8"
   },
   "source": [
    "Notice that, although our test set accuracy is lower with regularization the difference between target and test set are a much lower than in the solution without regularization. This will make your model a bit more stable when you want to productize it and use it with \"real\" new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgCau3Rt0LV9"
   },
   "source": [
    "Other ways to reduce overfit is to:\n",
    "    - Give less training data to the model (not optimum);\n",
    "    - Give less features to the model;\n",
    "    - Try another type of model;"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01 - Building a Sentiment Analysis Classification Model with Binary Features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
